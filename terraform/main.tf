/* main.tf: Deploys our service with the desired configuration */

# Terraform Config
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.0"
    }
  }
  required_version = "~> 1.3.0"

  # Our Remote State for Terraform on AWS
  # This bucket already exists in our AWS account
  backend "s3" {
    bucket = "banyan-tf-remote-state"
    key    = "dataprep-test/terraform.tfstate"
    region = "us-east-2"
  }
}

variable "aws_region" {
  default = "us-east-2"
}

# Provider Configuration
provider "aws" {
  region = "us-east-2"
}

data "aws_availability_zones" "available" {}

# Random Strings for passwords and such
# Create a random string to keep track of deployments
resource "random_string" "deploy_id" {
  length  = 8
  special = false
  upper   = false
}

# The name of our service
variable "service_name" {
  default = "dataprep-test"
}

# Environment variables for EBS configuration
variable "INPUT_DEVICE" {
  #  default     = "/dev/sdf"
  description = "The device name for the input set"
}

variable "PACKED_DEVICE" {
  #  default     = "/dev/sdg"
  description = "The device name for the packed set"
}

variable "UNPACKED_DEVICE" {
  #  default     = "/dev/sdh"
  description = "The device name for the unpacked set"
}

variable "INPUT_SIZE_GB" {
  #  default     = "125"
  description = "The size of a single input in GB"
}

variable "TOTAL_INPUT_SIZE_GB" {
  #  default     = "250"
  description = "The total size of the input set in GB"
}

/* Deploy the infrastructure for our service */
module "service" {
  source = "./service"

  # AWS configuration
  aws_region            = "us-east-2"
  aws_availability_zone = data.aws_availability_zones.available.names[0]
  # Service configuration
  name                  = var.service_name
  deploy_id             = random_string.deploy_id.result
  # Ec2 configuration
  # TODO: Replace with a larger instance type, something like r6g.8xlarge
  ec2_config            = {
    instance_type = "t3.medium" # Minimum instance type for our service
    monitoring    = "false"
    volume_type   = "gp2"
    volume_size   = "20" # Needed for building and storing results
  }
  # EBS configuration
  ebs_config = {
    # Cheap io optimized HDDs
    type                 = "st1"
    # Allocates 1.1 X the size of the set size for all sets
    # If an allocated size is less than 125 GiB, it will be set to 125 GiB
    input_device_name    = var.INPUT_DEVICE
    packed_device_name   = var.PACKED_DEVICE
    unpacked_device_name = var.UNPACKED_DEVICE
  }
  # Our Test Set configuration
  input_config = {
    size       = var.INPUT_SIZE_GB
    total_size = var.TOTAL_INPUT_SIZE_GB
  }
}
# This is a hack to make sure the Ansible inventory is generated after the EC2 instance is created
resource "null_resource" "ansible_inventory" {
  depends_on = [module.service]
  triggers   = {
    ec2_public_dns = module.service.ec2_public_dns
    ec2_pem_path   = module.service.ec2_pem_path
  }
  # Overwrite ../inventory/awshost with the public DNS of the EC2 instance and the path to the PEM file
  # Then write the public DNS and path to pem to ../pipeline_throughput.sh-aws-ssh to access in a shell
  provisioner "local-exec" {
    command = "echo '# This file is autogenerated by terraform' > ../env/env.ssh && echo 'EC2_PEM_PATH=${module.service.ec2_pem_path}' >> ../env/env.ssh && echo 'EC2_PUBLIC_DNS=${module.service.ec2_public_dns}' >> ../env/env.ssh && echo '${module.service.ec2_public_dns} ansible_ssh_private_key_file=${module.service.ec2_pem_path}' > ../inventory/awshost"
  }
}
# Outputs
output "ec2_public_dns" {
  value = module.service.ec2_public_dns
}
output "ec2_pem_path" {
  value = module.service.ec2_pem_path
}