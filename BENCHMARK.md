# dataprep-iac: Benchmark

## Benchmark Process Overview
This directory provides IaC meant for running `dataprep` on large amounts of data.
It is meant to be used a performance metric tool for realistic workflows.
As such it runs the whole `pack` and `unpack` process on a series of inputs, and measures the time it takes to complete.
It does *not* perform multiple iterations of the process on the same input -- it is not a statistically driven benchmark.
It is not necessarily meant as a developer tool for identifying performance bottlenecks, though it can (eventually) be used for that purpose once those types of benchmarks are added.
As a developer on `dataprep` you should:
- Develop and test your code locally
- Run project benchmarks on your local machine to get a baseline for performance using `cargo bench`
- If you observe performance gains, and want to verify them on a larger scale, run benchmarks on a remote machine:
  - Push your code to a branch on GitHub
  - Install the latest version of your branch on the remote machine (described below)
  - Run benchmarks on the remote machine using this repository (described below)
  - Compare and publicly share your results:
    - Pull the results of the benchmark from the remote machine (described below)
    - Compare the results of the benchmark on your local machine and the remote machine
    - If note-worthy, share your results with team.

## Supported instance types
It is recommended to run this tool on the Hetzner `AX101` host we maintain at `167.235.7.231`

In order to target this host, configure `env/env.host`. Set the following line:

```bash
ANSIBLE_INVENTORY=hosts/hetzner/ax101
```

## Setting up your environment 
The `AX101` host is already setup with the users, dependencies, and tools in order to run benchmarks:
- necessary system wide dependencies
- A `bench` user to use for running benchmarks (needed). 
  - If you're on the Banyan team you should be able to access this user with our SSH key (contact Alex if you need it)

Configure `env/env.user` to specify the `bench` user:

`USER=bench`


## Setting up benchmarks

### Installing Rust

The `bench` user already has rust installed. Should you need to reinstall it run:

```bash
./scripts/install/rust.sh
```

### Creating input to test with
The `bench` user already has a directory called `input` populated with a suite of test data.

This test input was generated by:
- configuring `env/env.input` with desired parameters (see that file for more info)
- running `./scripts/input/generate.sh`

### Setting up IFTTT
I use IFTTT to alert me when long running jobs finish.
If you would like to leverage that you should:
- create an IFTTT account
- create an applet called `dataprep_event`. This applet is triggered by a Webhook with JSON payload
- configure the applet to notify you using your preffered method. I use email.
- configure `env/env.ifttt` to specify your `IFTTT` webhook key

## (Finally) Running benchmarks 

### Installing the latest version of your branch + tools
The benchmark process uses the `dataprep` repository to handle installing the latest version of your branch on the instance, and to pull the results of the benchmark from the instance.

By default, the script install the `main` branch. If you would like to install a specific branch for benchmarking:
- configure `env/env.install` to reference the dataprep branch you want to test
- run `./scripts/install/dataprep`

This will install the specified branch of `dataprep` for the configured user 

### Running the Benchmark
Once you are ready to run the benchmark, run:

```bash
./script/dataprep/bench/run.sh
```

This will launch the benchmarking script in a new `tmux` session.

If you have set up IFTTT, you should recieve an alert when the job is finished

### Pulling the results of the benchmark
Results should be saved locally on the host. In order to pull them to your local machine run: 
```bash
# Copy the results of the benchmark to your working directory
./scripts/dataprep/bench/results.sh
```
You should see them organized by instance IP, commit hash of the dataprep branch tested, time run, and the input tested.
This data should appear in a directory called `results`